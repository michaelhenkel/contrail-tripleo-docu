<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Configuring Network Isolation &mdash; tripleo-docs 0.0.1.dev17 documentation</title>
    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="stylesheet" href="../_static/tweaks.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.0.1.dev17',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/cookies.js"></script>
    <script type="text/javascript" src="../_static/expandable.js"></script>
    <script type="text/javascript" src="../_static/admonition_selector.js"></script>
    <script type="text/javascript" src="../_static/jquery.scrollTo.js"></script>
    <script type="text/javascript" src="../_static/jquery.nav.js"></script>
    <script type="text/javascript" src="../_static/menu.js"></script>
    <link rel="top" title="tripleo-docs 0.0.1.dev17 documentation" href="../index.html" />
    <link rel="up" title="Advanced Deployment" href="advanced_deployment.html" />
    <link rel="next" title="Deploying Manila in the Overcloud" href="deploy_manila.html" />
    <link rel="prev" title="Deploying with Heat Templates" href="template_deploy.html" /> 
  </head>
  <body>
  <div id="header">
    <h1 id="logo"><a href="http://www.openstack.org/">OpenStack</a></h1>
    <ul id="navigation">
      
      <li><a href="http://www.openstack.org/" title="Go to the Home page" class="link">Home</a></li>
      <li><a href="http://www.openstack.org/projects/" title="Go to the OpenStack Projects page">Projects</a></li>
      <li><a href="http://www.openstack.org/user-stories/" title="Go to the User Stories page" class="link">User Stories</a></li>
      <li><a href="http://www.openstack.org/community/" title="Go to the Community page" class="link">Community</a></li>
      <li><a href="http://www.openstack.org/blog/" title="Go to the OpenStack Blog">Blog</a></li>
      <li><a href="http://wiki.openstack.org/" title="Go to the OpenStack Wiki">Wiki</a></li>
      <li><a href="http://docs.openstack.org/" title="Go to OpenStack Documentation" class="current">Documentation</a></li>
      
    </ul>
  </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="configuring-network-isolation">
<h1>Configuring Network Isolation<a class="headerlink" href="#configuring-network-isolation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>TripleO provides configuration of isolated overcloud networks. Using
this approach it is possible to host traffic for specific types of network
traffic (tenants, storage, API/RPC, etc.) in isolated networks. This allows
for assigning network traffic to specific network interfaces or bonds. Using
bonds provides fault tolerance, and may provide load sharing, depending on the
bonding protocols used. When isolated networks are configured, the OpenStack
services will be configured to use the isolated networks. If no isolated
networks are configured, all services run on the provisioning network.</p>
<p>There are two parts to the network configuration: the parameters that apply
to the network as a whole, and the templates which configure the network
interfaces on the deployed hosts.</p>
</div>
<div class="section" id="architecture">
<h2>Architecture<a class="headerlink" href="#architecture" title="Permalink to this headline">¶</a></h2>
<p>The following VLANs will be used in the final deployment:</p>
<ul class="simple">
<li>IPMI* (IPMI System controller, iLO, DRAC)</li>
<li>Provisioning* (Undercloud control plane for deployment and management)</li>
<li>Internal API (OpenStack internal API, RPC, and DB)</li>
<li>Tenant (Tenant tunneling network for GRE/VXLAN networks)</li>
<li>Storage (Access to storage resources from Compute and Controller nodes)</li>
<li>Storage Management (Replication, Ceph back-end services)</li>
<li>External (Public OpenStack APIs, Horzizon dashboard, optionally floating IPs)</li>
<li>Floating IP (Optional, can be combined with External)</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Networks marked with &#8216;*&#8217; are usually native VLANs, others may be trunked.</p>
</div>
<p>The External network should have a gateway router address. This will be used
in the subnet configuration of the network environment.</p>
<p>If floating IPs will be hosted on a separate VLAN from External, that VLAN will
need to be trunked to the controller hosts. It will not be included in the
network configuration steps for the deployment, the VLAN will be added via
Neutron and Open vSwitch. There can be multiple floating IP networks, and they
can be attached to multiple bridges. The VLANs will be trunked, but not
configured as interfaces. Instead, Neutron will create an OVS port with the
VLAN segmentation ID on the chosen bridge for each floating IP network.</p>
<p>The Provisioning network will usually be delivered on a dedicated interface.
DHCP+PXE is used to initially deploy, then the IP will be converted to static.
By default, PXE boot must occur on the native VLAN, although some system
controllers will allow booting from a VLAN. The Provisioning interface is
also used by the Compute and Storage nodes as their default gateway, in order
to contact DNS, NTP, and for system maintenance. The Undercloud can be used
as a default gateway, but in that case all traffic will be behind an IP
masquerade NAT, and will not be reachable from the rest of the network. The
Undercloud is also a single point of failure for the overcloud default route.
If there is an external gateway on a router device on the Provisioning network,
the Undercloud Neutron DHCP server can offer that instead. If the
<tt class="docutils literal"><span class="pre">network_gateway</span></tt> was not set properly in undercloud.conf, it can be set
manually after installing the Undercloud:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>neutron subnet-show     # Copy the UUID from the provisioning subnet
neutron subnet-update &lt;UUID&gt; --gateway_ip &lt;IP_ADDRESS&gt;
</pre></div>
</div>
<p>Often, the number of VLANs will exceed the number of physical Ethernet ports,
so some VLANs are delivered with VLAN tagging to separate the traffic. On an
Ethernet bond, typically all VLANs are trunked, and there is no traffic on the
native VLAN (native VLANs on bonds are supported, but will require customizing
the NIC templates).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">It is recommended to deploy a Tenant VLAN (which is used for tunneling GRE
and/or VXLAN) even if Neutron VLAN mode is chosen and tunneling is disabled
at deployment time. This requires the least customization at deployment time,
and leaves the option available to use tunnel networks as utility networks,
or for network function virtualization in the future. Tenant networks will
still be created using VLANs, but the operator can create VXLAN tunnels for
special use networks without consuming tenant VLANs. It is possible to add
VXLAN capability to a deployment with a Tenant VLAN, but it is not possible
to add a Tenant VLAN to an already deployed set of hosts without disruption.</p>
</div>
<p>The networks are connected to the roles as follows:</p>
<p>Controller:</p>
<ul class="simple">
<li>Provisioning</li>
<li>Internal API</li>
<li>Storage</li>
<li>Storage Management</li>
<li>External</li>
</ul>
<p>Compute:</p>
<ul class="simple">
<li>Provisioning</li>
<li>Internal API</li>
<li>Storage</li>
<li>Tenant</li>
</ul>
<p>Ceph Storage:</p>
<ul class="simple">
<li>Provisioning</li>
<li>Storage</li>
<li>Storage Management</li>
</ul>
<p>Cinder Storage:</p>
<ul class="simple">
<li>Provisioning</li>
<li>Internal API</li>
<li>Storage</li>
<li>Storage Management</li>
</ul>
<p>Swift Storage:</p>
<ul class="simple">
<li>Provisioning</li>
<li>Internal API</li>
<li>Storage</li>
<li>Storage Management</li>
</ul>
</div>
<div class="section" id="workflow">
<h2>Workflow<a class="headerlink" href="#workflow" title="Permalink to this headline">¶</a></h2>
<p>The procedure for enabling network isolation is this:</p>
<ol class="arabic simple">
<li>Create network environment file (e.g. /home/stack/network-environment.yaml)</li>
<li>Edit IP subnets and VLANs in the environment file to match local environment</li>
<li>Make a copy of the appropriate sample network interface configurations</li>
<li>Edit the network interface configurations to match local environment</li>
<li>Deploy overcloud with the proper parameters to include network isolation</li>
</ol>
<p>The next section will walk through the elements that need to be added to
the network-environment.yaml to enable network isolation. The sections
after that deal with configuring the network interface templates. The final step
will deploy the overcloud with network isolation and a custom environment.</p>
</div>
<div class="section" id="create-network-environment-file">
<h2>Create Network Environment File<a class="headerlink" href="#create-network-environment-file" title="Permalink to this headline">¶</a></h2>
<p>The environment file will describe the network environment and will point to
the network interface configuration files to use for the overcloud nodes.
The subnets that will be used for the isolated networks need to be defined,
along with the IP address ranges that should be used for IP assignment. These
values must be customized for the local environment.</p>
<p>It is important for the ExternalInterfaceDefaultRoute to be reachable on the
subnet that is used for ExternalNetCidr. This will allow the OpenStack Public
APIs and the Horizon Dashboard to be reachable. Without a valid default route,
the post-deployment steps cannot be performed.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <tt class="docutils literal"><span class="pre">resource_registry</span></tt> section of the network-environment.yaml contains
pointers to the network interface configurations for the deployed roles.
These files must exist at the path referenced here, and will be copied
later in this guide.</p>
</div>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>resource_registry:
  OS::TripleO::BlockStorage::Net::SoftwareConfig: /home/stack/nic-configs/cinder-storage.yaml
  OS::TripleO::Compute::Net::SoftwareConfig: /home/stack/nic-configs/compute.yaml
  OS::TripleO::Controller::Net::SoftwareConfig: /home/stack/nic-configs/controller.yaml
  OS::TripleO::ObjectStorage::Net::SoftwareConfig: /home/stack/nic-configs/swift-storage.yaml
  OS::TripleO::CephStorage::Net::SoftwareConfig: /home/stack/nic-configs/ceph-storage.yaml

parameter_defaults:
  # Customize all these values to match the local environment
  InternalApiNetCidr: 172.17.0.0/24
  StorageNetCidr: 172.18.0.0/24
  StorageMgmtNetCidr: 172.19.0.0/24
  TenantNetCidr: 172.16.0.0/24
  ExternalNetCidr: 10.1.2.0/24
  # CIDR subnet mask length for provisioning network
  ControlPlaneSubnetCidr: &#39;24&#39;
  InternalApiAllocationPools: [{&#39;start&#39;: &#39;172.17.0.10&#39;, &#39;end&#39;: &#39;172.17.0.200&#39;}]
  StorageAllocationPools: [{&#39;start&#39;: &#39;172.18.0.10&#39;, &#39;end&#39;: &#39;172.18.0.200&#39;}]
  StorageMgmtAllocationPools: [{&#39;start&#39;: &#39;172.19.0.10&#39;, &#39;end&#39;: &#39;172.19.0.200&#39;}]
  TenantAllocationPools: [{&#39;start&#39;: &#39;172.16.0.10&#39;, &#39;end&#39;: &#39;172.16.0.200&#39;}]
  # Use an External allocation pool which will leave room for floating IPs
  ExternalAllocationPools: [{&#39;start&#39;: &#39;10.1.2.10&#39;, &#39;end&#39;: &#39;10.1.2.50&#39;}]
  # Set to the router gateway on the external network
  ExternalInterfaceDefaultRoute: 10.1.2.1
  # Gateway router for the provisioning network (or Undercloud IP)
  ControlPlaneDefaultRoute: 192.0.2.254
  # Generally the IP of the Undercloud
  EC2MetadataIp: 192.0.2.1
  # Define the DNS servers (maximum 2) for the overcloud nodes
  DnsServers: [&quot;8.8.8.8&quot;,&quot;8.8.4.4&quot;]
  InternalApiNetworkVlanID: 201
  StorageNetworkVlanID: 202
  StorageMgmtNetworkVlanID: 203
  TenantNetworkVlanID: 204
  ExternalNetworkVlanID: 100
  # May set to br-ex if using floating IPs only on native VLAN on bridge br-ex
  NeutronExternalNetworkBridge: &quot;&#39;&#39;&quot;
  # Customize bonding options if required (ignored if bonds are not used)
  BondInterfaceOvsOptions:
      &quot;bond_mode=balance-tcp lacp=active other-config:lacp-fallback-ab=true&quot;
</pre></div>
</div>
</div>
<div class="section" id="configure-ip-subnets">
<h2>Configure IP Subnets<a class="headerlink" href="#configure-ip-subnets" title="Permalink to this headline">¶</a></h2>
<p>Each environment will have its own IP subnets for each network. This will vary
by deployment, and should be tailored to the environment. We will set the
subnet information for all the networks inside our environment file. Each
subnet will have a range of IP addresses that will be used for assigning IP
addresses to hosts and virtual IPs.</p>
<p>In the example above, the Allocation Pool for the Internal API network starts
at .10 and continues to .200. This results in the static IPs and virtual IPs
that are assigned starting at .10, and will be assigned upwards with .200 being
the highest assigned IP. The External network hosts the Horizon dashboard and
the OpenStack public API. If the External network will be used for both cloud
administration and floating IPs, we need to make sure there is room for a pool
of IPs to use as floating IPs for VM instances. Alternately, the floating IPs
can be placed on a separate VLAN (which is configured by the operator
post-deployment).</p>
</div>
<div class="section" id="configure-vlans-and-bonding-options">
<h2>Configure VLANs and Bonding Options<a class="headerlink" href="#configure-vlans-and-bonding-options" title="Permalink to this headline">¶</a></h2>
<p>The VLANs will need to be customized to match the environment. The values
entered in the <tt class="docutils literal"><span class="pre">network-environment.yaml</span></tt> will be used in the network
interface configuration templates covered below. For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span># Customize the VLAN IDs to match the local environment
InternalApiNetworkVlanID: 10
StorageNetworkVlanID: 20
StorageMgmtNetworkVlanID: 30
TenantNetworkVlanID: 40
ExternalNetworkVlanID: 50
</pre></div>
</div>
<p>The example bonding options will try to negotiate LACP, but will fallback to
active-backup if LACP cannot be established:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>BondInterfaceOvsOptions:
  &quot;bond_mode=balance-tcp lacp=active other-config:lacp-fallback-ab=true&quot;
</pre></div>
</div>
<p>The BondInterfaceOvsOptions parameter will pass the options to Open vSwitch
when setting up bonding (if used in the environment). The value above will
enable fault-tolerance and load balancing if the switch supports (and is
configured to use) LACP bonding. If LACP cannot be established, the bond will
fallback to active/backup mode, with fault tolerance, but where only one link
in the bond will be used at a time.</p>
<p>If the switches do not support LACP, then do not configure a bond on the
upstream switch. Instead, OVS can use <tt class="docutils literal"><span class="pre">balance-slb</span></tt> mode to enable using
two interfaces on the same VLAN as a bond:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Use balance-slb for bonds configured on a switch without LACP support</span>
<span class="s2">&quot;bond_mode=balance-slb lacp=off&quot;</span>
</pre></div>
</div>
<p>Bonding with balance-slb allows a limited form of load balancing without the
remote switch&#8217;s knowledge or cooperation. The basics of SLB are simple. SLB
assigns each source MAC+VLAN pair to a link and transmits all packets
from that MAC+VLAN through that link. Learning in the remote switch causes it
to send packets to that MAC+VLAN through the same link.</p>
<p>OVS will balance traffic based on source MAC and destination VLAN. The
switch will only see a given MAC address on one link in the bond at a time, and
OVS will use special filtering to prevent packet duplication across the links.</p>
<p>In addition, the following options may be added to the options string to tune
the bond:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Force bond to use active-backup, e.g. for connecting to 2 different switches</span>
<span class="s2">&quot;bond_mode=active-backup&quot;</span>

<span class="c1"># Set the LACP heartbeat to 1 second or 30 seconds (default)</span>
<span class="s2">&quot;other_config:lacp-time=[fast|slow]&quot;</span>

<span class="c1"># Set the link detection to use miimon heartbeats or monitor carrier (default)</span>
<span class="s2">&quot;other_config:bond-detect-mode=[miimon|carrier]&quot;</span>

<span class="c1"># If using miimon, heartbeat interval in milliseconds (100 is usually good)</span>
<span class="s2">&quot;other_config:bond-miimon-interval=100&quot;</span>

<span class="c1"># Number of milliseconds a link must be up to be activated (to prevent flapping)</span>
<span class="s2">&quot;other_config:bond_updelay=1000&quot;</span>

<span class="c1"># Milliseconds between rebalancing flows between bond members, zero to disable</span>
<span class="s2">&quot;other_config:bond-rebalance-interval=10000&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="creating-custom-interface-templates">
<h2>Creating Custom Interface Templates<a class="headerlink" href="#creating-custom-interface-templates" title="Permalink to this headline">¶</a></h2>
<p>In order to configure the network interfaces on each node, the network
interface templates may need to be customized.</p>
<p>Start by copying the configurations from one of the example directories. The
first example copies the templates which include network bonding. The second
example copies the templates which use a single network interface with
multiple VLANs (this configuration is mostly intended for testing).</p>
<p>To copy the bonded example interface configurations, run:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ cp /usr/share/openstack-tripleo-heat-templates/network/config/bond-with-vlans/* ~/nic-configs
</pre></div>
</div>
<p>To copy the single NIC with VLANs example interface configurations, run:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ cp /usr/share/openstack-tripleo-heat-templates/network/config/single-nic-vlans/* ~/nic-configs
</pre></div>
</div>
<p>Or, if you have custom NIC templates from another source, copy them to the location
referenced in the <tt class="docutils literal"><span class="pre">resource_registry</span></tt> section of the environment file.</p>
</div>
<div class="section" id="customizing-the-interface-templates">
<h2>Customizing the Interface Templates<a class="headerlink" href="#customizing-the-interface-templates" title="Permalink to this headline">¶</a></h2>
<p>The following example configures a bond on interfaces 3 and 4 of a system
with 4 interfaces. This example is based on the controller template from the
bond-with-vlans sample templates, but the bond has been placed on nic3 and nic4
instead of nic2 and nic3. The other roles will have a similar configuration,
but will have only a subset of the networks attached.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The nic1, nic2... abstraction considers only network interfaces which are
connected to an Ethernet switch. If interfaces 1 and 4 are the only
interfaces which are plugged in, they will be referred to as nic1 and nic2.</p>
</div>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>heat_template_version: 2015-04-30

description: &gt;
  Software Config to drive os-net-config with 2 bonded nics on a bridge
  with a VLANs attached for the controller role.

parameters:
  ControlPlaneIp:
    default: &#39;&#39;
    description: IP address/subnet on the ctlplane network
    type: string
  ExternalIpSubnet:
    default: &#39;&#39;
    description: IP address/subnet on the external network
    type: string
  InternalApiIpSubnet:
    default: &#39;&#39;
    description: IP address/subnet on the internal API network
    type: string
  StorageIpSubnet:
    default: &#39;&#39;
    description: IP address/subnet on the storage network
    type: string
  StorageMgmtIpSubnet:
    default: &#39;&#39;
    description: IP address/subnet on the storage mgmt network
    type: string
  TenantIpSubnet:
    default: &#39;&#39;
    description: IP address/subnet on the tenant network
    type: string
  BondInterfaceOvsOptions:
    default: &#39;&#39;
    description: The ovs_options string for the bond interface. Set things like
                 lacp=active and/or bond_mode=balance-slb using this option.
    type: string
  ExternalNetworkVlanID:
    default: 10
    description: Vlan ID for the external network traffic.
    type: number
  InternalApiNetworkVlanID:
    default: 20
    description: Vlan ID for the internal_api network traffic.
    type: number
  StorageNetworkVlanID:
    default: 30
    description: Vlan ID for the storage network traffic.
    type: number
  StorageMgmtNetworkVlanID:
    default: 40
    description: Vlan ID for the storage mgmt network traffic.
    type: number
  TenantNetworkVlanID:
    default: 50
    description: Vlan ID for the tenant network traffic.
    type: number
  ExternalInterfaceDefaultRoute:
    default: &#39;10.0.0.1&#39;
    description: Default route for the external network.
    type: string
  ControlPlaneSubnetCidr: # Override this via parameter_defaults
    default: &#39;24&#39;
    description: The subnet CIDR of the control plane network.
    type: string
  DnsServers: # Override this via parameter_defaults
    default: []
    description: A list of DNS servers (2 max) to add to resolv.conf.
    type: json
  EC2MetadataIp: # Override this via parameter_defaults
    description: The IP address of the EC2 metadata server.
    type: string

resources:
  OsNetConfigImpl:
    type: OS::Heat::StructuredConfig
    properties:
      group: os-apply-config
      config:
        os_net_config:
          network_config:
            -
              type: interface
              name: nic1
              use_dhcp: false
              addresses:
                -
                  ip_netmask:
                    list_join:
                      - &#39;/&#39;
                      - - {get_param: ControlPlaneIp}
                        - {get_param: ControlPlaneSubnetCidr}
              routes:
                -
                  ip_netmask: 169.254.169.254/32
                  next_hop: {get_param: EC2MetadataIp}
            -
              type: ovs_bridge
              name: {get_input: bridge_name}
              dns_servers: {get_param: DnsServers}
              members:
                -
                  type: ovs_bond
                  name: bond1
                  ovs_options: {get_param: BondInterfaceOvsOptions}
                  members:
                    -
                      type: interface
                      name: nic3
                      primary: true
                    -
                      type: interface
                      name: nic4
                -
                  type: vlan
                  device: bond1
                  vlan_id: {get_param: ExternalNetworkVlanID}
                  addresses:
                    -
                      ip_netmask: {get_param: ExternalIpSubnet}
                  routes:
                    -
                      ip_netmask: 0.0.0.0/0
                      next_hop: {get_param: ExternalInterfaceDefaultRoute}
                -
                  type: vlan
                  device: bond1
                  vlan_id: {get_param: InternalApiNetworkVlanID}
                  addresses:
                  -
                    ip_netmask: {get_param: InternalApiIpSubnet}
                -
                  type: vlan
                  device: bond1
                  vlan_id: {get_param: StorageNetworkVlanID}
                  addresses:
                  -
                    ip_netmask: {get_param: StorageIpSubnet}
                -
                  type: vlan
                  device: bond1
                  vlan_id: {get_param: StorageMgmtNetworkVlanID}
                  addresses:
                  -
                    ip_netmask: {get_param: StorageMgmtIpSubnet}
                -
                  type: vlan
                  device: bond1
                  vlan_id: {get_param: TenantNetworkVlanID}
                  addresses:
                  -
                    ip_netmask: {get_param: TenantIpSubnet}

outputs:
  OS::stack_id:
    description: The OsNetConfigImpl resource.
    value: {get_resource: OsNetConfigImpl}
</pre></div>
</div>
</div>
<div class="section" id="configuring-interfaces">
<h2>Configuring Interfaces<a class="headerlink" href="#configuring-interfaces" title="Permalink to this headline">¶</a></h2>
<p>The individual interfaces may need to be modified. As an example, below are
the modifications that would be required to use the second NIC to connect to
an infrastructure network with DHCP addresses, and to use the third and fourth
NICs for the bond:</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>network_config:
  # Add a DHCP infrastructure network to nic2
  -
    type: interface
    name: nic2
    use_dhcp: true
    defroute: false
  -
    type: ovs_bridge
    name: {get_input: bridge_name}
    members:
      -
        type: ovs_bond
        name: bond1
        ovs_options: {get_param: BondInterfaceOvsOptions}
        members:
          # Modify bond NICs to use nic3 and nic4
          -
            type: interface
            name: nic3
            primary: true
          -
            type: interface
            name: nic4
</pre></div>
</div>
<p>When using numbered interfaces (&#8220;nic1&#8221;, &#8220;nic2&#8221;, etc.) instead of named
interfaces (&#8220;eth0&#8221;, &#8220;eno2&#8221;, etc.), the network interfaces of hosts within
a role do not have to be exactly the same. For instance, one host may have
interfaces em1 and em2, while another has eno1 and eno2, but both hosts&#8217; NICs
can be referred to as nic1 and nic2.</p>
<p>The numbered NIC scheme only takes into account the interfaces that are live
(have a cable attached to the switch). So if you have some hosts with 4
interfaces, and some with 6, you should use nic1-nic4 and only plug in 4
cables on each host.</p>
</div>
<div class="section" id="configuring-routes-and-default-routes">
<h2>Configuring Routes and Default Routes<a class="headerlink" href="#configuring-routes-and-default-routes" title="Permalink to this headline">¶</a></h2>
<p>There are two ways that a host may have its default routes set. If the interface
is using DHCP, and the DHCP server offers a gateway address, the system will
install a default route for that gateway. Otherwise, a default route may be set
manually on an interface with a static IP.</p>
<p>Although the Linux kernel supports multiple default gateways, it will only use
the one with the lowest metric. If there are multiple DHCP interfaces, this can
result in an unpredictable default gateway. In this case, it is recommended that
defroute=no be set for the interfaces other than the one where we want the
default route. In this case, we want a DHCP interface (NIC 2) to be the default
route (rather than the Provisioning interface), so we disable the default route
on the provisioning interface (note that the defroute parameter only applies
to routes learned via DHCP):</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span># No default route on the Provisioning network
-
  type: interface
  name: nic1
  use_dhcp: true
  defroute: no
# Instead use this DHCP infrastructure VLAN as the default route
-
  type: interface
  name: nic2
  use_dhcp: true
</pre></div>
</div>
<p>To set a static route on an interface with a static IP, specify a route to the
subnet. For instance, here is a hypothetical route to the 10.1.2.0/24 subnet
via the gateway at 172.17.0.1 on the Internal API network:</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>-
  type: vlan
  device: bond1
  vlan_id: {get_param: InternalApiNetworkVlanID}
  addresses:
  -
    ip_netmask: {get_param: InternalApiIpSubnet}
  routes:
    -
      ip_netmask: 10.1.2.0/24
      next_hop: 172.17.0.1
</pre></div>
</div>
</div>
<div class="section" id="using-a-dedicated-interface-for-tenant-vlans">
<h2>Using a Dedicated Interface For Tenant VLANs<a class="headerlink" href="#using-a-dedicated-interface-for-tenant-vlans" title="Permalink to this headline">¶</a></h2>
<p>When using a dedicated interface or bond for tenant VLANs, a bridge must be
created. Neutron will create OVS ports on that bridge with the VLAN tags for the
provider VLANs. For example, to use NIC 4 as a dedicated interface for tenant
VLANs, you would add the following to the Controller and Compute templates:</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>-
  type: ovs_bridge
  name: br-vlan
  members:
    -
      type: interface
      name: nic4
      primary: true
</pre></div>
</div>
<p>A similar configuration may be used to define an interface or a bridge that
will be used for Provider VLANs. Provider VLANs are external networks which
are connected directly to the Compute hosts. VMs may be attached directly to
Provider networks to provide access to datacenter resources outside the cloud.</p>
</div>
<div class="section" id="using-the-native-vlan-for-floating-ips">
<h2>Using the Native VLAN for Floating IPs<a class="headerlink" href="#using-the-native-vlan-for-floating-ips" title="Permalink to this headline">¶</a></h2>
<p>By default, Neutron is configured with an empty string for the Neutron external
bridge mapping. This results in the physical interface being patched to br-int,
rather than using br-ex directly (as in previous versions). This model allows
for multiple floating IP networks, using either VLANs or multiple physical
connections.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>parameter_defaults:
  # May set to br-ex if using floating IPs only on native VLAN on bridge br-ex
  NeutronExternalNetworkBridge: &quot;&#39;&#39;&quot;
</pre></div>
</div>
<p>When using only one floating IP network on the native VLAN of a bridge,
then you can optionally set the Neutron external bridge to e.g. &#8220;br-ex&#8221;. This
results in the packets only having to traverse one bridge (instead of two),
and may result in slightly lower CPU when passing traffic over the floating
IP network.</p>
<p>The next section contains the changes to the NIC config that need to happen
to put the External network on the native VLAN (if the External network is on
br-ex, then that bridge may be used for floating IPs in addition to the Horizon
dashboard and Public APIs).</p>
</div>
<div class="section" id="using-the-native-vlan-on-a-trunked-interface">
<h2>Using the Native VLAN on a Trunked Interface<a class="headerlink" href="#using-the-native-vlan-on-a-trunked-interface" title="Permalink to this headline">¶</a></h2>
<p>If a trunked interface or bond has a network on the native VLAN, then the IP
address will be assigned directly to the bridge and there will be no VLAN
interface.</p>
<p>For example, if the external network is on the native VLAN, the bond
configuration would look like this:</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>-
  type: ovs_bridge
  name: {get_input: bridge_name}
  dns_servers: {get_param: DnsServers}
  addresses:
    -
      ip_netmask: {get_param: ExternalIpSubnet}
  routes:
    -
      ip_netmask: 0.0.0.0/0
      next_hop: {get_param: ExternalInterfaceDefaultRoute}
  members:
    -
      type: ovs_bond
      name: bond1
      ovs_options: {get_param: BondInterfaceOvsOptions}
      members:
        -
          type: interface
          name: nic3
          primary: true
        -
          type: interface
          name: nic4
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When moving the address (and possibly route) statements onto the bridge, be
sure to remove the corresponding VLAN interface from the bridge. Make sure to
make the changes to all applicable roles. The External network is only on the
controllers, so only the controller template needs to be changed. The Storage
network on the other hand is attached to all roles, so if the storage network
were on the default VLAN, all roles would need to be edited.</p>
</div>
</div>
<div class="section" id="configuring-jumbo-frames">
<h2>Configuring Jumbo Frames<a class="headerlink" href="#configuring-jumbo-frames" title="Permalink to this headline">¶</a></h2>
<p>The Maximum Transmission Unit (MTU) setting determines the maximum amount of
data that can be transmitted by a single Ethernet frame. Using a larger value
can result in less overhead, since each frame adds data in the form of a
header. The default value is 1500, and using a value higher than that will
require the switch port to be configured to support jumbo frames. Most switches
support an MTU of at least 9000, but many are configured for 1500 by default.</p>
<p>The MTU of a VLAN cannot exceed the MTU of the physical interface. Make sure to
include the MTU value on the bond and/or interface.</p>
<p>Storage, Storage Management, Internal API, and Tenant networking can all
benefit from jumbo frames. In testing, tenant networking throughput was
over 300% greater when using jumbo frames in conjunction with VXLAN tunnels.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">It is recommended that the Provisioning interface, External interface, and
any floating IP interfaces be left at the default MTU of 1500. Connectivity
problems are likely to occur otherwise. This is because routers typically
cannot forward jumbo frames across L3 boundaries.</p>
</div>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>-
  type: ovs_bond
  name: bond1
  mtu: 9000
  ovs_options: {get_param: BondInterfaceOvsOptions}
  members:
    -
      type: interface
      name: nic3
      mtu: 9000
      primary: true
    -
      type: interface
      name: nic4
      mtu: 9000
-
  # The external interface should stay at default
  type: vlan
  device: bond1
  vlan_id: {get_param: ExternalNetworkVlanID}
  addresses:
    -
      ip_netmask: {get_param: ExternalIpSubnet}
  routes:
    -
      ip_netmask: 0.0.0.0/0
      next_hop: {get_param: ExternalInterfaceDefaultRoute}
-
  # MTU 9000 for Internal API, Storage, and Storage Management
  type: vlan
  device: bond1
  mtu: 9000
  vlan_id: {get_param: InternalApiNetworkVlanID}
  addresses:
  -
    ip_netmask: {get_param: InternalApiIpSubnet}
</pre></div>
</div>
</div>
<div class="section" id="assigning-openstack-services-to-isolated-networks">
<h2>Assigning OpenStack Services to Isolated Networks<a class="headerlink" href="#assigning-openstack-services-to-isolated-networks" title="Permalink to this headline">¶</a></h2>
<p>Each OpenStack service is assigned to a network using a default mapping. The
service will be bound to the host IP within the named network on each host.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The services will be assigned to the networks according to the
<tt class="docutils literal"><span class="pre">ServiceNetMap</span></tt> in <tt class="docutils literal"><span class="pre">overcloud.yaml</span></tt>. Unless these
defaults need to be overridden, the ServiceNetMap does not need to be defined
in the environment file.</p>
</div>
<p>A service can be assigned to an alternate network by overriding the service to
network map in an environment file. The defaults should generally work, but
can be overridden. To override these values, add the ServiceNetMap to the
<tt class="docutils literal"><span class="pre">parameter_defaults</span></tt> section of the network environment.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>parameter_defaults:

  ServiceNetMap:
    NeutronTenantNetwork: tenant
    CeilometerApiNetwork: internal_api
    MongoDbNetwork: internal_api
    CinderApiNetwork: internal_api
    CinderIscsiNetwork: storage
    GlanceApiNetwork: storage
    GlanceRegistryNetwork: internal_api
    KeystoneAdminApiNetwork: internal_api
    KeystonePublicApiNetwork: internal_api
    NeutronApiNetwork: internal_api
    HeatApiNetwork: internal_api
    NovaApiNetwork: internal_api
    NovaMetadataNetwork: internal_api
    NovaVncProxyNetwork: internal_api
    SwiftMgmtNetwork: storage_mgmt
    SwiftProxyNetwork: storage
    HorizonNetwork: internal_api
    MemcachedNetwork: internal_api
    RabbitMqNetwork: internal_api
    RedisNetwork: internal_api
    MysqlNetwork: internal_api
    CephClusterNetwork: storage_mgmt
    CephPublicNetwork: storage
    # Define which network will be used for hostname resolution
    ControllerHostnameResolveNetwork: internal_api
    ComputeHostnameResolveNetwork: internal_api
    BlockStorageHostnameResolveNetwork: internal_api
    ObjectStorageHostnameResolveNetwork: internal_api
    CephStorageHostnameResolveNetwork: storage
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If an entry in the ServiceNetMap points to a network which does not exist,
that service will be placed on the Provisioning network. To avoid that,
make sure that each entry points to a valid network.</p>
</div>
</div>
<div class="section" id="updating-existing-configuration-templates-to-support-new-parameters">
<h2>Updating Existing Configuration Templates To Support New Parameters<a class="headerlink" href="#updating-existing-configuration-templates-to-support-new-parameters" title="Permalink to this headline">¶</a></h2>
<p>The most recent versions of TripleO include support for static Provisioning IPs.
The systems will boot via DHCP during deployment, and the DHCP address assigned
is converted to a static IP. The following parameters have been added to support
static IP addressing on the provisioning network:</p>
<ul class="simple">
<li>ControlPlaneIp</li>
<li>ControlPlaneSubnetCidr</li>
<li>DnsServers</li>
<li>EC2MetadataIp</li>
</ul>
<p>These changes require additional parameters for setting static IPs, routes,
and DNS servers. When using static Provisioning IPs, the network environment
file now needs to contain additional resource defaults (customize to match
the environment):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>parameter_defaults:
  # CIDR subnet mask length for provisioning network
  ControlPlaneSubnetCidr: &#39;24&#39;
  # Gateway router for the provisioning network (or Undercloud IP)
  ControlPlaneDefaultRoute:10.8.146.254
  # Generally the IP of the Undercloud
  EC2MetadataIp: 10.8.146.1
  # Define the DNS servers (maximum 2) for the overcloud nodes
  DnsServers:[&#39;8.8.8.8&#39;,&#39;8.8.4.4&#39;]
</pre></div>
</div>
<p>The NIC config templates for each role now include additional parameters in the
parameters section. Whether the provisioning interface will use DHCP or static
IPs, these parameters are needed in any case:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>parameters:
  ControlPlaneIp:
    default: &#39;&#39;
    description: IP address/subnet on the ctlplane network
    type: string
  ControlPlaneSubnetCidr: # Override this via parameter_defaults
    default: &#39;24&#39;
    description: The subnet CIDR of the control plane network.
    type: string
  DnsServers: # Override this via parameter_defaults
    default: []
    description: A list of DNS servers (2 max) to add to resolv.conf.
    type: json
  EC2MetadataIp: # Override this via parameter_defaults
    description: The IP address of the EC2 metadata server.
    type: string
</pre></div>
</div>
<p>If you are customizing the templates in the <tt class="docutils literal"><span class="pre">network/config</span></tt> subdirectory of
the TripleO Heat Templates, you will find that they have been updated with
these parameters. If you have NIC configuration templates from an older version
of TripleO Heat Templates, then you will need to add these parameters and
modify the provisioning network to take advantage of static IP addresses.</p>
</div>
<div class="section" id="deploying-the-overcloud-with-network-isolation">
<h2>Deploying the Overcloud With Network Isolation<a class="headerlink" href="#deploying-the-overcloud-with-network-isolation" title="Permalink to this headline">¶</a></h2>
<p>When deploying with network isolation, you should specify the NTP server for the
overcloud nodes. If the clocks are not synchronized, some OpenStack services may
be unable to start, especially when using HA. The NTP server should be reachable
from both the External and Provisioning subnets. The neutron network type should
be specified, along with the tunneling or VLAN parameters. Specify the libvirt
type if on bare metal, so that hardware virtualization will be used.</p>
<p>To deploy with network isolation and include the network environment file, use
the <tt class="docutils literal"><span class="pre">-e</span></tt> parameters with the <tt class="docutils literal"><span class="pre">openstack</span> <span class="pre">overcloud</span> <span class="pre">deploy</span></tt> command. For
instance, to deploy VXLAN mode, the deployment command might be:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>openstack overcloud deploy --templates \
-e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \
-e /home/stack/templates/network-environment.yaml \
--ntp-server pool.ntp.org \
--neutron-network-type vxlan \
--neutron-tunnel-types vxlan
</pre></div>
</div>
<p>To deploy with VLAN mode, you should specify the range of VLANs that will be
used for tenant networks:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>openstack overcloud deploy --templates \
-e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \
-e /home/stack/templates/network-environment.yaml \
--ntp-server pool.ntp.org \
--neutron-network-type vlan \
--neutron-bridge-mappings datacentre:br-ex \
--neutron-network-vlan-ranges datacentre:30:100
</pre></div>
</div>
<p>If a dedicated interface or bridge is used for tenant VLANs or provider
networks, it should be included in the bridge mappings. For instance, if the
tenant VLANs were on a bridge named <tt class="docutils literal"><span class="pre">br-vlan</span></tt>, then use these values in the
deployment command above:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>--neutron-bridge-mappings datacentre:br-ex,tenant:br-vlan \
--neutron-network-vlan-ranges tenant:30:100
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You must also pass the environment files (again using the <tt class="docutils literal"><span class="pre">-e</span></tt> or
<tt class="docutils literal"><span class="pre">--environment-file</span></tt> option) whenever you make subsequent changes to the
overcloud, such as <a class="reference internal" href="../post_deployment/scale_roles.html"><em>Scaling overcloud roles</em></a>,
<a class="reference internal" href="../post_deployment/delete_nodes.html"><em>Deleting Overcloud Nodes</em></a> or
<a class="reference internal" href="../post_deployment/package_update.html"><em>Updating Packages on Overcloud Nodes</em></a>.</p>
</div>
</div>
<div class="section" id="creating-floating-ip-networks">
<h2>Creating Floating IP Networks<a class="headerlink" href="#creating-floating-ip-networks" title="Permalink to this headline">¶</a></h2>
<p>In order to provide external connectivity and floating IPs to the VMs, an
external network must be created. The physical network is referred to by the
name used in the Neutron bridge mappings when deployed. The default bridge
mapping is <tt class="docutils literal"><span class="pre">datacentre:br-ex</span></tt>, which maps the physical network name
<tt class="docutils literal"><span class="pre">datacentre</span></tt> to the bridge <tt class="docutils literal"><span class="pre">br-ex</span></tt> which includes the physical network
link. For instance, to create a floating IP network on the br-ex bridge on
VLAN 104, this command is used:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>neutron net-create ext-net --router:external \
--provider:physical_network datacentre \
--provider:network_type vlan \
--provider:segmentation_id 104
</pre></div>
</div>
<p>If the floating IP network is on the native VLAN of br-ex, then a different
command is used to create the external network:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>neutron net-create ext-net --router:external \
--provider:physical_network datacentre \
--provider:network_type flat
</pre></div>
</div>
<p>Floating IP networks do not have to use br-ex, they can use any bridge as
long as the NeutronExternalNetworkBridge is set to &#8220;&#8217;&#8216;&#8221;. If the floating IP
network were going to be placed on a bridge named &#8220;br-floating&#8221;, and the
deployment command included the bridge mapping of
<tt class="docutils literal"><span class="pre">datacenter:br-ex,floating:br-floating</span></tt>, then following command would be used
to create a floating IP network on VLAN 105:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>neutron net-create ext-net --router:external \
    --provider:physical_network floating \
    --provider:network_type vlan \
    --provider:segmentation_id 105
</pre></div>
</div>
<p>Then a range of IP addresses must be assigned in the floating IP subnet and
assigned to the physical network. The Subnet will be associated with the network
name that was created in the previous step (<tt class="docutils literal"><span class="pre">ext-net</span></tt>):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>neutron subnet-create --name ext-subnet \
--enable_dhcp=False \
--allocation-pool start=10.8.148.50,end=10.8.148.100 \
--gateway 10.8.148.254 \
ext-net 10.8.148.0/24
</pre></div>
</div>
</div>
<div class="section" id="creating-provider-networks">
<h2>Creating Provider Networks<a class="headerlink" href="#creating-provider-networks" title="Permalink to this headline">¶</a></h2>
<p>A Provider Network is a network which is attached physically to a datacenter
network that exists outside of the deployed overcloud. This can be an existing
infrastructure network, or a network which provides external access directly to
VMs via routing instead of floating IPs.</p>
<p>When a provider network is created, it is associated with a physical network
with a bridge mapping, similar to how floating IP networks are created. The
provider network being added must be attached to both the controller and the
compute nodes, since the compute node will attach a VM virtual network
interface directly to an attached network interface.</p>
<p>For instance, if the provider network being added is a VLAN on the br-ex
bridge, then this command would add a provider network on VLAN 201:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>neutron net-create --provider:physical_network datacentre \
--provider:network_type vlan --provider:segmentation_id 201 \
--shared provider_network
</pre></div>
</div>
<p>This command would create a shared network, but it is also possible to
specify a tenant instead of specifying &#8211;shared, and then that network will
only be available to that tenant. If a provider network is marked as external,
then only the operator may create ports on that network. A subnet can be added
to a provider network if Neutron is to provide DHCP services to tenant VMs:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>neutron subnet-create --name provider-subnet \
--enable_dhcp=True \
--allocation-pool start=10.9.101.50,end=10.9.101.100 \
--gateway 10.9.101.254 \
provider_network 10.9.101.0/24
</pre></div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
<div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
            <h3>Table Of Contents</h3>
            <ul><li><a href="../index.html">Return to project home page</a></li></ul>
            <ul>
<li><a class="reference internal" href="#">Configuring Network Isolation</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#architecture">Architecture</a></li>
<li><a class="reference internal" href="#workflow">Workflow</a></li>
<li><a class="reference internal" href="#create-network-environment-file">Create Network Environment File</a></li>
<li><a class="reference internal" href="#configure-ip-subnets">Configure IP Subnets</a></li>
<li><a class="reference internal" href="#configure-vlans-and-bonding-options">Configure VLANs and Bonding Options</a></li>
<li><a class="reference internal" href="#creating-custom-interface-templates">Creating Custom Interface Templates</a></li>
<li><a class="reference internal" href="#customizing-the-interface-templates">Customizing the Interface Templates</a></li>
<li><a class="reference internal" href="#configuring-interfaces">Configuring Interfaces</a></li>
<li><a class="reference internal" href="#configuring-routes-and-default-routes">Configuring Routes and Default Routes</a></li>
<li><a class="reference internal" href="#using-a-dedicated-interface-for-tenant-vlans">Using a Dedicated Interface For Tenant VLANs</a></li>
<li><a class="reference internal" href="#using-the-native-vlan-for-floating-ips">Using the Native VLAN for Floating IPs</a></li>
<li><a class="reference internal" href="#using-the-native-vlan-on-a-trunked-interface">Using the Native VLAN on a Trunked Interface</a></li>
<li><a class="reference internal" href="#configuring-jumbo-frames">Configuring Jumbo Frames</a></li>
<li><a class="reference internal" href="#assigning-openstack-services-to-isolated-networks">Assigning OpenStack Services to Isolated Networks</a></li>
<li><a class="reference internal" href="#updating-existing-configuration-templates-to-support-new-parameters">Updating Existing Configuration Templates To Support New Parameters</a></li>
<li><a class="reference internal" href="#deploying-the-overcloud-with-network-isolation">Deploying the Overcloud With Network Isolation</a></li>
<li><a class="reference internal" href="#creating-floating-ip-networks">Creating Floating IP Networks</a></li>
<li><a class="reference internal" href="#creating-provider-networks">Creating Provider Networks</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="template_deploy.html"
                                  title="previous chapter">Deploying with Heat Templates</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="deploy_manila.html"
                                  title="next chapter">Deploying Manila in the Overcloud</a></p>
            <h3>Project Source</h3>
            <ul class="this-page-menu">
              <li><a href="http://git.openstack.org/cgit/michaelhenkel/contrail-tripleo-docu
"
                     rel="nofollow">Project Source</a></li>
            </ul>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="../_sources/advanced_deployment/network_isolation.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
  <div id="admonition_selector">
    <span class="trigger">Limit Environment Specific Content</span>

    <div class="content">
      <span class="title">Operating Systems</span>
      <ul>
        <li><input type="checkbox" id="centos" checked="checked"><label for="centos" title="Step that should only be run when using CentOS.">CentOS</label></li>
        <li><input type="checkbox" id="rhel" checked="checked"><label for="rhel" title="Step that should only be run when using RHEL.">RHEL</label></li>
      </ul>

      <span class="title">RHEL Registration Types</span>
      <ul>
        <li><input type="checkbox" id="portal" checked="checked"><label for="portal" title="Step that should only be run when registering to the Red Hat Portal.">Portal</label></li>
        <li><input type="checkbox" id="satellite" checked="checked"><label for="satellite" title="Step that should only be run when registering to Red Hat Satellite.">Satellite</label></li>
      </ul>

      <span class="title">Environments</span>
      <ul>
        <li><input type="checkbox" id="baremetal" checked="checked"><label for="baremetal" title="Step that should only be run when deploying to baremetal.">Baremetal</label></li>
        <li><input type="checkbox" id="virtual" checked="checked"><label for="virtual" title="Step that should only be run when deploying to virtual machines.">Virtual</label></li>
        <li><input type="checkbox" id="ssl" checked="checked"><label for="ssl" title="Step that should only be run when deploying with SSL OpenStack endpoints.">SSL</label></li>
        <li><input type="checkbox" id="selfsigned" checked="checked"><label for="selfsigned" title="Step that should only be run when deploying with SSL and a self-signed certificate.">Self-Signed SSL</label></li>
      </ul>

      <span class="title">Additional Overcloud Roles</span>
      <ul>
        <li><input type="checkbox" id="ceph" checked="checked"><label for="ceph" title="Step that should only be run when deploying Ceph for use by the Overcloud.">Ceph</label></li>
      </ul>

      <span class="title">Development options</span>
      <ul>
         <li><input type="checkbox" id="source" checked=""><label for="source"
            title="Step that should only be run when choosing to use some components directly from their git source code repositories instead of packages.">Install from source</label></li>
        <li><input type="checkbox" id="stable" checked=""><label for="stable"
            title="Step that should only be run when choosing to use components from their stable branches rather than using packages/source based on current master.">Install from stable branch</label></li>
      </ul>


    </div>
  </div>

  
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="../search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>

    </div>
</div>

      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="deploy_manila.html" title="Deploying Manila in the Overcloud"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="template_deploy.html" title="Deploying with Heat Templates"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">tripleo-docs 0.0.1.dev17 documentation</a> &raquo;</li>
          <li><a href="advanced_deployment.html" accesskey="U">Advanced Deployment</a> &raquo;</li> 
      </ul>
    </div>

    <div class="footer">
        &copy; Copyright 2015, OpenStack Foundation.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.3.
    </div>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
//Tracking docs.openstack.org/developer/<projectname> only
//The URL is built from the project variable in conf.py
var pageTracker = _gat._getTracker("UA-17511903-1");
pageTracker._setCookiePath("/developer/tripleo-docs");
pageTracker._trackPageview();
} catch(err) {}</script>

  </body>
</html>